cmResults <- read.csv("Data and Dependencies/cmResults.csv")
#install.packages('raster')
library(rstudioapi)
library(Rcpp)
library(ggplot2)
library(Hmisc)
library(randomForest)
library(snowfall)
library(caret)
library(doParallel)
library(RSNNS)
library(e1071)
library(klaR)
library(nnet)
library(raster)
my.mlog <- function(train,test){
#Train MLogit Reg
fit.mlog <- multinom(y~., data=train)
print(t(coef(fit.mlog)))
#Predict on trained MLogit Reg
return(predict(fit.mlog, newdata = test[,1:6]))
}
my.NB <- function(train,test){
#Train NBC
fit.NB <- naiveBayes(y~., data=train)
#Predict off trained NBC
return(predict(fit.NB, newdata = test[,1:6], type='class'))
}
my.knn <- function(train,test){
clus <- parallel::makeCluster(spec=6, type = 'PSOCK')
print(clus)
registerDoParallel
#Train KNN with k ranging from 5 to 100 and choose k w/ max accuracy
fit.knn <- caret::train(y~., data=train, method='knn',
tuneGrid=expand.grid(.k=5:100), metric =
"accuracy", trControl=ctrl.train)
stopCluster(clus)
#Predict using trained knn
return(predict(fit.knn$finalModel, newdata=test[,1:6],
type='class'))
}
#Predict off trained MLP
probs.nnet <- predict(fit.nnet$finalModel, newdata=test[,1:6])
head(probs.nnet)
#Original predict output is type class probabilities
#For test, coerce prediction result to categorical
out.nnet <- apply(probs.nnet,1,which.max)
return(factor(out.nnet, levels=1:3, labels = levels(test$y)))
}
#MLP classifier with single layer of hidden neurons
my.nnet <- function(train,test){
clus <- parallel::makeCluster(spec=6, type='PSOCK')
registerDoParallel(clus)
#Train MLP with varied number of hidden neurons and store best result
fit.nnet <- caret::train(y~., data=train, method='mlp', metric='Accuracy',
tuneGrid=expand.grid(.size=1:15), learnFunc='SCG',
trControl=ctrl.train)
stopCluster(clus)
#Describe optimal NNET structure
summary(fit.nnet$finalModel)
#Predict off trained MLP
probs.nnet <- predict(fit.nnet$finalModel, newdata=test[,1:6])
head(probs.nnet)
#Original predict output is type class probabilities
#For test, coerce prediction result to categorical
out.nnet <- apply(probs.nnet,1,which.max)
return(factor(out.nnet, levels=1:3, labels = levels(test$y)))
}
#Make rf classifier
my.rf <- function(train, test){
clus <- parallel::makeCluster(spec=6, type='PSOCK')
registerDoParallel(clus)
#Train the RF with varied mrty and store best result based on accuracy
fit.rf <- caret::train(y ~ ., data=train, method='rf', metric='Accuracy',
tuneGrid=expand.grid(.mtry=1:6), trControl=ctrl.train,
ntree=1000)
stopCluster(clus)
#output of RFs with varying mtry parameter
print(fit.rf)
plot(fit.rf)
#Predict on test with trained RF model
return(predict(fit.rf$finalModel, newdata = test[,1:6], type='class'))
}
#Prepare acc/kappa data to proper df format for plot
acc_kap_box <- data.frame("model"=factor(rep(1:5,200),labels=c("RF",
"NNET","KNN","NBayes","MLogit")),"metric"=factor(
c(rep(1,nrow(cmResults)),rep(2,nrow(cmResults))),
labels=c("Accuracy","Kappa")),"perf"=c(cmResults$acc,
cmResults$kappa))
#Create box/whisker plot with accuracy and kappa
acc_kap_plot <- ggplot(aes(y=perf,x=model), data=acc_kap_box)+
geom_boxplot(aes(fill=model))+
coord_flip()+
facet_wrap(~metric,ncol=1,scales="fixed")+
scale_fill_discrete(guide=F)+
scale_x_discrete(name="")+scale_y_continuous(name="")+
theme(text=element_text(size=24))
print(acc_kap_plot)
#Prepare acc/kappa data to proper df format for plot
sens_box <- data.frame("model"=factor(rep(1:5,300),labels=c("RF",
"NNET","KNN","NBayes","MLogit")),"metric"=factor(
c(rep(1,nrow(cmResults)),rep(2,nrow(cmResults)),
rep(3,nrow(cmResults))),labels=c("Sensitivity W",
"Sensitivity L","Sensitivity D")),"perf"=c(cmResults$sensW,
cmResults$sensL,cmResults$sensD))
#Box/whisker plot for sensitivity
sens_plot <- ggplot(aes(y=perf,x=model), data=sens_box)+
geom_boxplot(aes(fill=model))+
coord_flip()+
facet_wrap(~metric,ncol=1,scales="fixed")+
scale_fill_discrete(guide=F)+
scale_x_discrete(name="")+scale_y_continuous(name="")+
theme(text=element_text(size=24))
print(sens_plot)
#create table of mean, standard deviation. and coefficient of
#variation from results accuracy and kappa
accKap <- cbind(aggregate(cmResults[,c('acc','kappa')],list(cmResults$model),mean),
aggregate(cmResults[,c('acc','kappa')],list(cmResults$model),sd)[,2:3],
aggregate(cmResults[,c('acc','kappa')],list(cmResults$model),cv)[,2:3])
#Label
rownames(accKap)  <- accKap$Group.1
accKap            <- accKap[,2:ncol(accKap)]
colnames(accKap)  <- c("Acc.AVG","Kappa.AVG","Acc.STDV","Kappa.STDV",
"Acc.CV","Kappa.CV")
#Compare accuracy and kappa average, standard deviation, coeff of var
print(accKap)
#create table of mean, standard deviation. and coefficient of
#variation from results of Win, Loss, and Draw Sensitivities
sens <- cbind(aggregate(cmResults[,c('sensW','sensL','sensD')],list(cmResults$model),mean),
aggregate(cmResults[,c('sensW','sensL','sensD')],list(cmResults$model),sd)[,2:4],
aggregate(cmResults[,c('sensW','sensL','sensD')],list(cmResults$model),cv)[,2:4])
#Label
rownames(sens)  <- sens$Group.1
sens            <- sens[,2:ncol(sens)]
colnames(sens)  <- c("SensW.AVG","SensL.AVG",'SensD.avg',"SensW.STDV","SensL.STDV",
'SensD.STDV',"SensW.CV","SensL.CV",'SensD.CV')
#Compare accuracy and kappa average, standard deviation, coeff of var
print(sens)
#As we compare results, it looks like MLogit has the best accuracry and spread
#For sensitivitiy, the field is pretty even but it looks like MLogit
#still slightly outperforms the other models so we will use
#MLogit going forward as it is the best balance between accuracy,
#simplicity, and stability
#Additionally, it is interesting that MLogit, an intrinsicly linear
#model outperforms more complex models. This alludes to the data
#following at least somewhat of a linear trend.
memory.limit()
memory.limit(size=12000)
memory.limit(size=18000)
install.packages('compiler')
install.packages("compiler")
install.packages("compiler")
install.packages("compiler")
install.packages('compiler')
install.packages("compiler")
library(compiler)
mean_r = function(x){
m = 0
n = length(x)
for(i in seq_len(n)){
m = m + x[i]/n
m
}
}
cmp_mean_r = cmpfun(mean_r)
x = rnorm(1000)
install.packages('microbenchmark')
library(microbenchmark)
microbenchmark(times=10, unit="ms", mean_r(x), cmp_mean_r(x),
mean(x))
install.packages('sqldf')
library(rstudioapi)
library(sqldf)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(iris)
load(iris)
library(iris)
library(Iris)
data(iris)
write.csv(iris,"Data and Dependencies/iris.csv", quote = FALSE,row.names = FALSE)
iris_ex <- read.csv.sql("Data and Dependencies/iris.csv")
iris_ex <- read.csv.sql("Data and Dependencies/iris.csv",
sql="SELECT * FROM file WHERE Species = 'setosa'")
View(iris_ex)
library(rstudioapi)
install.packages('hflights')
library(hflights)
library(hflights)
data("hflights")
write.csv(hflihgts, "Data and Dependencies/hflights.csv", row.names=FALSE)
write.csv(hflights, "Data and Dependencies/hflights.csv", row.names=FALSE)
DecJFKflgihts <- read.csv.sql("Data and Dependencies/hflights.csv",
sql="SELECT * FROM file where Dest='\"JFK|"' and Month=12")
DecJFKflgihts <- read.csv.sql("Data and Dependencies/hflights.csv",
sql="SELECT * FROM file where Dest='\"JFK\"' and Month=12")
